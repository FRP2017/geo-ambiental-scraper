import streamlit as st
from google.cloud import bigquery
from scraper import ejecutar_scrapping

st.set_page_config(page_title="SEIA Document Crawler", layout="wide")

@st.cache_data(ttl=3600)
def obtener_proyectos_bq():
    try:
        client = bigquery.Client()
        # Seleccionamos los 3 campos necesarios para identificar de forma √∫nica
        query = """
                SELECT DISTINCT 
                TRIM(REPLACE(nombre_proyecto, '(e-seia)', '')) AS nombre_proyecto, 
                titular, 
                fecha_presentacion
            FROM `geo-ambiental-481814.dataset_ambiental.raw_seia_final` 
            WHERE nombre_proyecto IS NOT NULL
            ORDER BY nombre_proyecto ASC
        """
        query_job = client.query(query)
        df = query_job.to_dataframe()
        return df
    except Exception as e:
        st.error(f"Error al conectar con BigQuery: {e}")
        return None

st.title("üï∑Ô∏è SEIA: Crawler de Documentos")
st.markdown("Extrae autom√°ticamente el Excel, la ficha y el contenido √≠ntegro (HTML/PDF) usando b√∫squeda por metadatos √∫nicos.")

with st.spinner("Cargando listado de proyectos √∫nicos..."):
    df_proyectos = obtener_proyectos_bq()

if df_proyectos is not None:
    # Creamos una lista de opciones que muestra los 3 metadatos
    opciones = []
    for _, row in df_proyectos.iterrows():
        fecha_fmt = row['fecha_presentacion'].strftime('%d/%m/%Y')
        label = f"{row['nombre_proyecto']} | {row['titular']} | {fecha_fmt}"
        opciones.append({
            "label": label,
            "nombre": row['nombre_proyecto'],
            "titular": row['titular'],
            "fecha": row['fecha_presentacion']
        })

    seleccion = st.selectbox(
        "Selecciona el proyecto exacto para procesar:",
        options=opciones,
        format_func=lambda x: x["label"] if x else "Seleccione un proyecto..."
    )

    st.divider()

    if st.button("üöÄ Iniciar Crawling Profundo", use_container_width=True):
        if seleccion:
            with st.status(f"Procesando: {seleccion['nombre']}...", expanded=True) as status:
                st.write(f"üîç Buscando por Nombre, Titular y Fecha: {seleccion['fecha']}")
                
                # Pasamos los 3 par√°metros al scraper
                resultado, logs = ejecutar_scrapping(
                    seleccion['nombre'], 
                    seleccion['titular'], 
                    seleccion['fecha']
                )
                
                if "‚úÖ EXITOSO" in resultado:
                    partes = resultado.split("|")
                    status.update(label=f"‚úÖ ¬°Completado! {partes[4]} documentos procesados.", state="complete")
                    
                    st.success(f"**Proceso finalizado.** Se ha evitado la duplicidad mediante b√∫squeda por metadatos.")
                    
                    col1, col2 = st.columns(2)
                    with col1:
                        st.metric("Documentos", partes[4])
                    with col2:
                        st.metric("Criterio Selecci√≥n", "Metadatos √önicos")

                    st.info(f"üìç **Bucket:** `{partes[2]}`")
                    st.link_button("üìÇ Explorar Archivos", partes[3])
                else:
                    status.update(label="‚ùå Fall√≥ el proceso", state="error")
                    st.error(resultado)
                
                with st.expander("Ver bit√°cora de ejecuci√≥n (Logs)"):
                    st.code(logs)
        else:
            st.warning("Selecciona un proyecto.")
else:
    st.error("No se pudo cargar la base de datos de proyectos.")